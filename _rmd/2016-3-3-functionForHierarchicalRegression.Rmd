---
title: "A function to build hierarchical regression models"
author: "Simon A Jackson"
date: "3 March 2016"
output: 
  html_document: 
    keep_md: yes
    self_contained: no
---

Understanding hierarchical regression is essential for any good data modeller, particularly those working in scientific disciplines like psychology (my background). But building them in R can get fiddly, leaving you with the difficult task of tracking multiple objects that might contain errors. In this post, I'll demonstrate how I construct hierarchical regression models using strings and lists to reduce this complexity and uncertainty.

# Data and modelling question

We'll use the `mtcars` data provided with R for this post. This data set provides us with 32 observations scored on eleven continous variables:

```{r}
str(mtcars)
```

For this example, we'll predict the `mpg` (Miles per gallon) for cars with their `cyl`	(Number of cylinders), `hp`	(Gross horsepower), and `wt` (Weight [lb/1000]), and their interaction terms. A convention when investigating interaction terms in regression is to determine whether they significantly add to the predictive power of the model over and above the main effect terms. This is achieved by comparing nested/hierarchical regression models.

As we plan on looking at interaction terms, we should start by mean centering our variables. Let's create a copy of the `mtcars` data set with all numeric variables centered just to be safe.

```{r}
dat <- lapply(mtcars, scale, scale = FALSE)  # Centre but no need to standardise
dat <- as.data.frame(dat)
summary(dat)
```

# The first approach

We want to know if adding the interaction terms to the regression provides a statistically significant improvement in model fit (measured by $R^{2}$). A typical approach is to write out spearate models like this:

```{r}
# Model 1: main effects only
fit.1 <- lm(mpg ~ cyl + hp + wt, dat)

# Model 2: + 2-way interaction terms
fit.2 <- lm(mpg ~ cyl + hp + wt +
                    cyl:hp + cyl:wt + hp:wt, dat)

# Model 3: + 3-way interaction term
fit.3 <- lm(mpg ~ cyl + hp + wt +
                    cyl:hp + cyl:wt + hp:wt +
                    cyl:hp:wt, dat)
```

We can examine their results by calling summary on each model.

```{r}
summary(fit.1)
summary(fit.2)
summary(fit.3)
```

We then determine whether each more complex model has a significantly different $R^{2}$ to the less complex model using `anova()`.

```{r}
anova(fit.1, fit.2, fit.3)
```

It appears that the two-way interactions add, but the three-way interactions do not. Still, this is not important right now. What is important is the process by which we achieved this result.

So far it seems straight forward. But it doesn't seem to scale very well. What if you have a large data set with tens of predictors, and want four, five, or many more nested models to compare against each other? The first immediate problem is that you have a lot of R code to type out! Every model has to be a perfect copy of the last, plus the new variables. Then you end up with objects for every model, which need to be named, summarised and compared separately. Put simply, things can get messy fast!

# Using strings

The regression function `lm()` accepts strings. Let's check:

```{r}
model.1 <- "mpg ~ cyl + hp + wt"
fit.1   <- lm(model.1, dat)
summary(fit.1)
```

OK, this is cool, but why bother? The main reason is that we can create our models as string, and iteratively concantenate on them to ensure that we have nested models! Let's try.

```{r}
model.2 <- paste(model.1, "cyl:hp", "cyl:wt", "hp:wt", sep = " + ")  # Paste on to model 1
model.3 <- paste(model.2, "cyl:hp:wt", sep = " + ")  # Paste on to model 2
print(model.2)
print(model.3)

fit.2   <- lm(model.2, dat)
fit.3   <- lm(model.3, dat)

summary(fit.2)
summary(fit.3)

anova(fit.1, fit.2, fit.3)
```

Seems to work and look a bit neater, but there is still room for error. It's a fine point, but it's easy for us to accidentally forget to change the model and fit numbers as we move along. E.g., Model 2 has to paste on Model 1, Model 3 has to paste on Model 2, and so on. If we have lots of models, it's easy for us to get this wrong. Also, we still have lots of objects. In fact, we've got even more objects this time because we have 'model.X' for the each model string, and then 'fit.X' for each model fit.

# Using lists

To manage these issues, I like to construct hierarchical models as strings in a list. The main idea is that we grow a list with the simplest model as the first object, next as the second, and so on. This way, we can always paste on to the last model in the list by referencing its index with `length(our.model.list)`. So we can always reuse the same code, regardless of what model number we're up to. Let's give it a shot.

```{r}
models <- list()
models[[1]] <- "mpg ~ cyl + hp + wt"  # Start with model 1

# To add a new model, we get the number of models in our list with
# length(models) and add 1. We then paste new variables onto the last model,
# again with length(models).
models[[length(models) + 1]] <- paste(models[[length(models)]], "cyl:hp", "cyl:wt", "hp:wt", sep = " + ")
models[[length(models) + 1]] <- paste(models[[length(models)]], "cyl:hp:wt", sep = " + ")

print(models)
```

We can create a list of corresponding model fits using `lapply()` to iterate through each model and run the regression using our data frame (`dat`) as the data.

```{r}
fits <- lapply(models, lm, dat)  # Fit all models
```

We can print out a model summary for the *n*th model:
```{r}
summary(fits[[2]])  # Summary for model 2
```

We can also use `lapply()` to print out all of the model summaries at once.

```{r}
lapply(fits, summary)
```

To iteratively compare all our model fits, we can use `do.call()` and `anova()` together.

```{r}
do.call(anova, fits)
```

Great! So we now have two objects: `models` and `fits`. We created `models` as a list and in a way that we couldn't make mistakes.
Once we have `models`, we only need one line of code to fit all of the regression models (`fits <- lapply(models, lm, dat)`). After this, we only need one line of code to see all of their summaries (`lapply(fits, summary)`) or compare all of the models together (`do.call(anova, fits)`).

# As a function

This looks like a improvement over our first approach, but there's one final thing I'd like to do. Right now, building the models seems a little convoluted. It works, but it's a lot of repeated code. So let's write a function to handle the model building. There are a few ways we can do this. For clarity, I'll write a function that builds the whole string list of nested regression models at once. As arguements, we take the dependent/outcome variable (dv), and then vectors of the independent variables (ivs) to be included in each new model. We then paste these together as needed.

```{r}
#' Create a list of hierarchical regression models
createHierarchical <- function(dv, ivs.1, ...) {
  
  ivs <- list(ivs.1, ...)  # ivs for each model as a list
  nModels <- length(ivs)  # Number of models (foruse later)
  models <- vector("list", nModels)  # empty list of required length

  # Create base model
  models[[1]] <- paste(dv, paste(ivs[[1]], collapse = " + "),
                       sep = " ~ ")
  
  # Create all other models
  if (nModels >= 2) {
    for (i in 2:nModels) {
      models[[i]] <- paste(models[[i - 1]],
                           paste(ivs[[i]], collapse = " + "),
                           sep = " + ")
    }
  }

  return (models)
}
```

Let's give it a whirl.

```{r}
models <- createHierarchical("mpg",  # DV
                       c("cyl", "hp", "wt"),  # IVs in model 1
                       c("cyl:hp", "cyl:wt", "hp:wt"),  # IVs to add in model 2
                       c("cyl:hp:wt"))  # IVs to add in model 3
print(models)
fits <- lapply(models, lm, dat)
do.call(anova, fits)
```

Thanks to our utility function `createHierarchical()`, we can build, fit, and analyse our hierarchical regression in a transparent and simple way. If you wanted, you could go as far as to write another function to fit the models for us, thus removing the need for a `models` object. However, I'll leave this up to you if you're interested. I still like having the ability to quickly look at the models if I need to.

Let's try something a little more convulated by adding only one variable at a time to make sure that this function scales up.

```{r}
models <- createHierarchical("mpg", "cyl", "hp", "wt", "cyl:hp", "cyl:wt", "hp:wt", "cyl:hp:wt")
fits <- lapply(models, lm, dat)
do.call(anova, fits)
```

We just constructed, fit, and compared seven nested regression models in three lines of code! As before, you're able to look at the individual model summaries with `summary(fits[[model.number]])` or look at them all at once with `lapply(fits, summary)`.

# Sign-off 

As always, what's presented here is just my approach. It's not the only approach, and it's unlikley to be the best approach. I hope that you're able to glean something useful from it. Please comment, email me at <drsimonjackson@gmail.com>, or tweet @drsimonj to chat!